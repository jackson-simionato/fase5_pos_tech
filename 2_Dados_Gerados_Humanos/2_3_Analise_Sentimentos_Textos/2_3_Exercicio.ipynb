{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data_tweet</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wed Jan 04 21:43:51 +0000 2017</td>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                      data_tweet  \\\n",
       "0   0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1   1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2   2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "3   3  Wed Jan 04 21:43:51 +0000 2017   \n",
       "4   4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "\n",
       "                                               texto sentimento  \n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...     Neutro  \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...     Neutro  \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...     Neutro  \n",
       "3                        ��� https://t.co/BnDsO34qK0     Neutro  \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...   Negativo  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, string, unicodedata, nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df = pd.read_csv('../../data/bases_disciplina_2/tweets_classificados.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 1\n",
    "\n",
    "Altere as funções de tratamento de texto apresentadas em sala para que elas façam a remoção de links também.\n",
    "\n",
    "Crie uma nova coluna chamada texto_tratado que conterá o resultado da aplicação das funções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove acentos\n",
    "def normalize_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "# Remove pontuação\n",
    "def remove_punctuaction(text):\n",
    "    punctiations = string.punctuation\n",
    "    table = str.maketrans({key: \" \" for key in punctiations})\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "# Remove URLs\n",
    "def remove_links(text):\n",
    "    return re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "\n",
    "# Converte para minusculua, aplica funções de normalização e retira espaços em branco adicionais\n",
    "def normalize_str(text):\n",
    "    text = text.lower()\n",
    "    text = remove_links(text)\n",
    "    text = remove_punctuaction(text)\n",
    "    text = normalize_accents(text)\n",
    "    text = re.sub(re.compile(r\" +\"),\" \", text)\n",
    "    return ' '.join([w for w in text.split()])\n",
    "\n",
    "# Função completa de tokenização com exclusão de stopwords e verificação de dtype\n",
    "def tokenizer(text):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)\n",
    "        text = ''.join([w for w in text if not w.isdigit()])\n",
    "        text = word_tokenize(text)\n",
    "        text = [x for x in text if x not in stop_words]\n",
    "        text = [y for y in text if len(y) >= 2]\n",
    "        return ' '.join([t for t in text])\n",
    "    else:\n",
    "        print('Not a string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meu link'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Meu link é asdasd(https://twitterr34321)sadasd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 2\n",
    "\n",
    "Ao fazer a remoção de links, percebemos que algumas linhas da coluna texto_tratado possuem valores faltantes. Entretanto, o Python trata eles como ''(str) e nao como Null. Assim, um simples dropna nao resolve o problema.\n",
    "\n",
    "Encontre uma forma de remover tais elementos. Dica: use o índice das linhas cujos elementos da coluna texto_tratado seja nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_norm'] = df['texto'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "data_tweet     0\n",
       "texto          0\n",
       "sentimento     0\n",
       "texto_norm    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['texto_norm'] == '', 'texto_norm'] = None\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['texto_norm'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5763, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 3\n",
    "\n",
    "Separe a coluna texto_tratado em conjunto de treino e teste na proporção 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data_tweet</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>texto_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>catedral de santo antonio governador valadares mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>psol vai questionar aumento de vereadores pref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Sat Jan 07 13:47:55 +0000 2017</td>\n",
       "      <td>\" bom é bandido morto\"\\nDeputado Cabo Júlio é ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>bom bandido morto deputado cabo julio condenad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                      data_tweet  \\\n",
       "0   0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1   1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2   2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "4   4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "5   5  Sat Jan 07 13:47:55 +0000 2017   \n",
       "\n",
       "                                               texto sentimento  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...     Neutro   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...     Neutro   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...     Neutro   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...   Negativo   \n",
       "5  \" bom é bandido morto\"\\nDeputado Cabo Júlio é ...     Neutro   \n",
       "\n",
       "                                          texto_norm  \n",
       "0  catedral de santo antonio governador valadares mg  \n",
       "1                  governador valadares minas gerais  \n",
       "2                  governador valadares minas gerais  \n",
       "4  psol vai questionar aumento de vereadores pref...  \n",
       "5  bom bandido morto deputado cabo julio condenad...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['texto_norm']\n",
    "y = df['sentimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4034,)\n",
      "(1729,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 4\n",
    "\n",
    "Transforme os dados para criar a representação numérica dos textos. Use uma versão com CountVectorizer e outra com TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4034x5539 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Set\n",
    "vect = CountVectorizer(ngram_range=(1,1), lowercase=False)\n",
    "vect.fit(X_train)\n",
    "X_train_bow = vect.transform(X_train)\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1729x5539 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19040 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set\n",
    "X_test_bow = vect.transform(X_test)\n",
    "X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1360)\t2\n",
      "  (0, 1642)\t1\n",
      "  (0, 1666)\t1\n",
      "  (0, 1739)\t1\n",
      "  (0, 2539)\t1\n",
      "  (0, 2830)\t1\n",
      "  (0, 3313)\t1\n",
      "  (0, 3569)\t1\n",
      "  (0, 4072)\t1\n",
      "  (0, 4179)\t1\n",
      "  (0, 4728)\t1\n",
      "  (0, 5175)\t1\n"
     ]
    }
   ],
   "source": [
    "# Representação BoW\n",
    "print(X_train_bow[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4034x5539 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 47392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Set\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(X_train)\n",
    "X_train_tfidf = vect.transform(X_train)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1729x5539 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19040 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set\n",
    "X_test_tfidf = vect.transform(X_test)\n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5175)\t0.2629330229264354\n",
      "  (0, 4727)\t0.2678863366110196\n",
      "  (0, 4178)\t0.2783295967143665\n",
      "  (0, 4071)\t0.25477993961651946\n",
      "  (0, 3568)\t0.4269274910984725\n",
      "  (0, 3312)\t0.16380757215655944\n",
      "  (0, 2829)\t0.4601614616161445\n",
      "  (0, 2538)\t0.35735858271161447\n",
      "  (0, 1738)\t0.12469597980884331\n",
      "  (0, 1665)\t0.204676106532364\n",
      "  (0, 1641)\t0.2686589766811486\n",
      "  (0, 1359)\t0.19634200164711335\n"
     ]
    }
   ],
   "source": [
    "# Representação TF-IDF\n",
    "print(X_train_tfidf[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo5\n",
    "\n",
    "Treine uma árvore de decisão nas duas abordagens e compare seus resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1360)\t2\n",
      "  (0, 1642)\t1\n",
      "  (0, 1666)\t1\n",
      "  (0, 1739)\t1\n",
      "  (0, 2539)\t1\n",
      "  (0, 2830)\t1\n",
      "  (0, 3313)\t1\n",
      "  (0, 3569)\t1\n",
      "  (0, 4072)\t1\n",
      "  (0, 4179)\t1\n",
      "  (0, 4728)\t1\n",
      "  (0, 5175)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bow[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "360 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.90902125 0.89985395 0.89910861\n",
      " 0.86836951 0.86216696 0.8740684  0.83762149 0.82621356 0.82053004\n",
      " 0.88447978 0.89588464 0.88770621 0.68715673 0.6430495  0.64947128\n",
      " 0.52775959 0.50198204 0.5205694         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64898423 0.7265481  0.65393778 0.68220902 0.71093164 0.60385799\n",
      " 0.65844118 0.6586392  0.64827794 0.53718979 0.53345848 0.5669474\n",
      " 0.55132664 0.52627875 0.52578278 0.52503928 0.52007896 0.56073593\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73772635 0.69435369 0.79101626\n",
      " 0.75683581 0.70722709 0.81505561 0.72582244 0.75931966 0.74217348\n",
      " 0.59447299 0.57139791 0.6170318  0.57534876 0.57116422 0.55083712\n",
      " 0.53793759 0.5292475  0.53719163        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.85249876 0.82623324 0.83936954 0.73403317 0.76625341 0.79252416\n",
      " 0.78260444 0.76500318 0.77317762 0.63562285 0.6234542  0.59865261\n",
      " 0.59444224 0.59721051 0.61777069 0.54164737 0.5465917  0.52429486\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.89861448 0.90555222 0.90505994\n",
      " 0.85101423 0.85770353 0.83936246 0.80811694 0.81160442 0.80491297\n",
      " 0.87729513 0.88844878 0.88200301 0.64574397 0.62966567 0.65765649\n",
      " 0.5924479  0.53743055 0.52676764        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65021508 0.64452388 0.61526254 0.63413217 0.6859105  0.65223002\n",
      " 0.63858084 0.61700075 0.6680485  0.51412824 0.52752344 0.51834168\n",
      " 0.57262538 0.52726577 0.53421427 0.52751698 0.5074374  0.51388533\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.74365708 0.75137676 0.72683129\n",
      " 0.71591687 0.71391546 0.78009969 0.78085179 0.74491899 0.69732736\n",
      " 0.61326237 0.60980472 0.55529163 0.59569523 0.55700923 0.58848906\n",
      " 0.55874436 0.52478253 0.53098385        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82126    0.80590399 0.8245058  0.79350688 0.80936502 0.79745465\n",
      " 0.76969937 0.80689716 0.76673308 0.59468239 0.62740598 0.59345122\n",
      " 0.62441878 0.61897141 0.58031615 0.56840087 0.52281771 0.53073141]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - BoW: 0.9034123770965876\n"
     ]
    }
   ],
   "source": [
    "dt_classifier_bow = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "dt_classifier_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = dt_classifier_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy - BoW:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "360 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.91100513 0.9010931  0.90753334\n",
      " 0.86488296 0.86712359 0.86787262 0.82101279 0.83540147 0.82722579\n",
      " 0.8812549  0.88844939 0.88721331 0.65767586 0.67528911 0.67328832\n",
      " 0.52255205 0.52800434 0.57461911        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70255488 0.64328595 0.65149606 0.60260807 0.58997451 0.67698703\n",
      " 0.69036747 0.68098708 0.65794214 0.55279487 0.53023851 0.53742501\n",
      " 0.55283976 0.53916629 0.5277685  0.53098232 0.52008019 0.5178451\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.81433241 0.80936286 0.72781247\n",
      " 0.77070423 0.77368589 0.75086203 0.75582512 0.72087319 0.76722321\n",
      " 0.59989054 0.60387921 0.54065574 0.5642068  0.55231089 0.55874098\n",
      " 0.53173504 0.52851384 0.56222476        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84209138 0.80838691 0.81207917 0.79920423 0.81581325 0.76051946\n",
      " 0.8175496  0.81135566 0.83663417 0.59195931 0.59494836 0.62421\n",
      " 0.6246703  0.61604601 0.56643452 0.56864993 0.51833891 0.56542843\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.89440381 0.89613278 0.89141968\n",
      " 0.86365425 0.8542385  0.84580977 0.83118802 0.83639279 0.80489329\n",
      " 0.8832394  0.87481712 0.87878058 0.70475738 0.67104277 0.66631491\n",
      " 0.54090265 0.56196371 0.53221748        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69536069 0.64451558 0.63759966 0.67030296 0.63137713 0.74468315\n",
      " 0.65070583 0.64920869 0.64400823 0.53371492 0.53693242 0.55580513\n",
      " 0.52875153 0.54684445 0.53395629 0.55923172 0.555763   0.51462882\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.74269282 0.73404208 0.80837277\n",
      " 0.70897113 0.74466655 0.76028086 0.74242377 0.78507784 0.74814511\n",
      " 0.57957512 0.58626688 0.56148096 0.57559444 0.56493646 0.54908078\n",
      " 0.5232977  0.53669812 0.55527964        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.83662925 0.81208932 0.8264866  0.76401616 0.75210826 0.79396687\n",
      " 0.79525154 0.7972225  0.787562   0.61254378 0.60039512 0.61352588\n",
      " 0.57635423 0.57487001 0.58007693 0.53943196 0.5394052  0.52429948]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - TFIDF: 0.9161364950838635\n"
     ]
    }
   ],
   "source": [
    "dt_classifier_tfidf = DecisionTreeClassifier(max_depth=20, min_samples_split=2, random_state=42)\n",
    "\n",
    "dt_classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = dt_classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy - TFIDF:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 6\n",
    "\n",
    "Crie uma função que lematiza as palavras da coluna texto_tratado apenas se elas forem um verbo. Depois, crie uma nova coluna chamada texto_tratado_lemma que conterá o resultado da aplicação da função na coluna texto_tratado.\n",
    "\n",
    "Dica: use o Corpus pt_core_news_sm como referência para determinar a classe gramatical da palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(text):\n",
    "    text = nlp(text)\n",
    "    text_lemmatized = [token.lemma_ if token.pos_ == 'VERB' else token.orth_ for token in text]\n",
    "    return ' '.join([t for t in text_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_norm_lemma'] = df['texto_norm'].apply(lemmatize_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data_tweet</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>texto_norm</th>\n",
       "      <th>texto_norm_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>catedral de santo antonio governador valadares mg</td>\n",
       "      <td>catedral de santo antonio governador valadares mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "      <td>governador valadares minas gerais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>psol vai questionar aumento de vereadores pref...</td>\n",
       "      <td>psol vai questionar aumento de vereadores pref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Sat Jan 07 13:47:55 +0000 2017</td>\n",
       "      <td>\" bom é bandido morto\"\\nDeputado Cabo Júlio é ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>bom bandido morto deputado cabo julio condenad...</td>\n",
       "      <td>bom bandido morto deputado cabo julio condenar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>8194</td>\n",
       "      <td>Thu Feb 09 11:48:07 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>trio preso suspeito de roubo trafico abuso sex...</td>\n",
       "      <td>trio prender suspeito de roubo trafico abuso s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>8195</td>\n",
       "      <td>Thu Feb 09 12:10:19 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>trio preso suspeito de roubo trafico abuso sex...</td>\n",
       "      <td>trio prender suspeito de roubo trafico abuso s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>8196</td>\n",
       "      <td>Thu Feb 09 12:04:17 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>trio preso suspeito de roubo trafico abuso sex...</td>\n",
       "      <td>trio prender suspeito de roubo trafico abuso s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>8197</td>\n",
       "      <td>Thu Feb 09 12:10:04 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>trio preso suspeito de roubo trafico abuso sex...</td>\n",
       "      <td>trio prender suspeito de roubo trafico abuso s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5774</th>\n",
       "      <td>8198</td>\n",
       "      <td>Tue Feb 07 15:01:30 +0000 2017</td>\n",
       "      <td>Trio suspeito de roubo de cargas é preso em Sa...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>trio suspeito de roubo de cargas preso em sant...</td>\n",
       "      <td>trio suspeito de roubo de cargas prender em sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5763 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                      data_tweet  \\\n",
       "0        0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1        1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2        2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "4        4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "5        5  Sat Jan 07 13:47:55 +0000 2017   \n",
       "...    ...                             ...   \n",
       "5770  8194  Thu Feb 09 11:48:07 +0000 2017   \n",
       "5771  8195  Thu Feb 09 12:10:19 +0000 2017   \n",
       "5772  8196  Thu Feb 09 12:04:17 +0000 2017   \n",
       "5773  8197  Thu Feb 09 12:10:04 +0000 2017   \n",
       "5774  8198  Tue Feb 07 15:01:30 +0000 2017   \n",
       "\n",
       "                                                  texto sentimento  \\\n",
       "0     ���⛪ @ Catedral de Santo Antônio - Governador ...     Neutro   \n",
       "1     � @ Governador Valadares, Minas Gerais https:/...     Neutro   \n",
       "2     �� @ Governador Valadares, Minas Gerais https:...     Neutro   \n",
       "4     ��� PSOL vai questionar aumento de vereadores ...   Negativo   \n",
       "5     \" bom é bandido morto\"\\nDeputado Cabo Júlio é ...     Neutro   \n",
       "...                                                 ...        ...   \n",
       "5770  Trio é preso suspeito de roubo, tráfico e abus...   Positivo   \n",
       "5771  Trio é preso suspeito de roubo, tráfico e abus...   Positivo   \n",
       "5772  Trio é preso suspeito de roubo, tráfico e abus...   Positivo   \n",
       "5773  Trio é preso suspeito de roubo, tráfico e abus...   Positivo   \n",
       "5774  Trio suspeito de roubo de cargas é preso em Sa...   Positivo   \n",
       "\n",
       "                                             texto_norm  \\\n",
       "0     catedral de santo antonio governador valadares mg   \n",
       "1                     governador valadares minas gerais   \n",
       "2                     governador valadares minas gerais   \n",
       "4     psol vai questionar aumento de vereadores pref...   \n",
       "5     bom bandido morto deputado cabo julio condenad...   \n",
       "...                                                 ...   \n",
       "5770  trio preso suspeito de roubo trafico abuso sex...   \n",
       "5771  trio preso suspeito de roubo trafico abuso sex...   \n",
       "5772  trio preso suspeito de roubo trafico abuso sex...   \n",
       "5773  trio preso suspeito de roubo trafico abuso sex...   \n",
       "5774  trio suspeito de roubo de cargas preso em sant...   \n",
       "\n",
       "                                       texto_norm_lemma  \n",
       "0     catedral de santo antonio governador valadares mg  \n",
       "1                     governador valadares minas gerais  \n",
       "2                     governador valadares minas gerais  \n",
       "4     psol vai questionar aumento de vereadores pref...  \n",
       "5     bom bandido morto deputado cabo julio condenar...  \n",
       "...                                                 ...  \n",
       "5770  trio prender suspeito de roubo trafico abuso s...  \n",
       "5771  trio prender suspeito de roubo trafico abuso s...  \n",
       "5772  trio prender suspeito de roubo trafico abuso s...  \n",
       "5773  trio prender suspeito de roubo trafico abuso s...  \n",
       "5774  trio suspeito de roubo de cargas prender em sa...  \n",
       "\n",
       "[5763 rows x 6 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meu', 'DET'),\n",
       " ('texto', 'NOUN'),\n",
       " ('gostaria', 'VERB'),\n",
       " ('de', 'SCONJ'),\n",
       " ('ser', 'AUX'),\n",
       " ('melhor', 'ADJ'),\n",
       " ('escrito', 'VERB')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [(token.orth_, token.pos_) for token in texto]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meu texto gostar de ser melhor escrever'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = [token.lemma_ if token.pos_ == 'VERB' else token.orth_ for token in texto]\n",
    "' '.join(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo 7\n",
    "\n",
    "repita os ToDo 3, ToDo 4 e ToDo 5, usando como feature a coluna texto_tratado_lemma, e veja se os resultados tiveram melhora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['texto_norm_lemma']\n",
    "y = df['sentimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4034x5287 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47405 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Set\n",
    "vect = CountVectorizer(ngram_range=(1,1), lowercase=False)\n",
    "vect.fit(X_train)\n",
    "X_train_bow = vect.transform(X_train)\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1729x5287 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19132 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set\n",
    "X_test_bow = vect.transform(X_test)\n",
    "X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1338)\t2\n",
      "  (0, 1599)\t1\n",
      "  (0, 1625)\t1\n",
      "  (0, 1697)\t1\n",
      "  (0, 2442)\t1\n",
      "  (0, 2719)\t1\n",
      "  (0, 3183)\t1\n",
      "  (0, 3431)\t1\n",
      "  (0, 3907)\t1\n",
      "  (0, 3987)\t1\n",
      "  (0, 4512)\t1\n",
      "  (0, 4944)\t1\n"
     ]
    }
   ],
   "source": [
    "# Representação BoW\n",
    "print(X_train_bow[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "360 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jackson Simionato\\.conda\\envs\\pyspark_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.90704075 0.90282669 0.89985149\n",
      " 0.86614026 0.87580814 0.86440544 0.82770824 0.82770824 0.81829187\n",
      " 0.87977498 0.87208821 0.87431962 0.64627991 0.64578394 0.64503983\n",
      " 0.55823978 0.55823978 0.55823978        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66658918 0.67105107 0.66683793 0.66237512 0.66782772 0.66658672\n",
      " 0.66510065 0.66510065 0.66510065 0.58701129 0.58676346 0.58676346\n",
      " 0.58651532 0.58651532 0.58651532 0.55278749 0.55278749 0.55278749\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.77540964 0.76896418 0.78358716\n",
      " 0.77391804 0.77416587 0.77268135 0.76871389 0.76871389 0.76920955\n",
      " 0.60089016 0.59841216 0.60039512 0.59717269 0.59791619 0.59742052\n",
      " 0.5599746  0.5599746  0.5599746         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.8408513  0.83366449 0.83069482 0.81978224 0.81432872 0.82052543\n",
      " 0.80292478 0.80292478 0.80044647 0.61205211 0.61576835 0.61130893\n",
      " 0.60536066 0.60511283 0.60734331 0.55823978 0.55823978 0.55823978\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.90505902 0.90654417 0.90109003\n",
      " 0.86142254 0.85894207 0.85795013 0.84159203 0.84159203 0.84010104\n",
      " 0.88522574 0.87655071 0.87084967 0.63884097 0.64008167 0.64181741\n",
      " 0.55724846 0.55724846 0.55724846        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64676851 0.64726417 0.64701664 0.64627192 0.64602378 0.64627161\n",
      " 0.64552812 0.64552812 0.64528029 0.58701129 0.58676346 0.58676346\n",
      " 0.58651532 0.58651532 0.58651532 0.55278749 0.55278749 0.55278749\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73476036 0.73005218 0.73773619\n",
      " 0.72484649 0.72658131 0.73104258 0.7250934  0.7250934  0.7258372\n",
      " 0.59593384 0.5964295  0.59667703 0.59320739 0.59320739 0.59320739\n",
      " 0.55650496 0.55650496 0.55650496        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.77764966 0.77640712 0.79102795 0.77813794 0.78656329 0.7828452\n",
      " 0.77267735 0.77267735 0.77342761 0.61229779 0.61378417 0.60758746\n",
      " 0.60040065 0.60411812 0.6023833  0.55724846 0.55724846 0.55724846]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - BoW: 0.9224985540775015\n"
     ]
    }
   ],
   "source": [
    "dt_classifier_bow = DecisionTreeClassifier(max_depth=30, min_samples_split=2, random_state=42)\n",
    "\n",
    "dt_classifier_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = dt_classifier_bow.predict(X_test_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy - BoW:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4034x5268 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 47401 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Set\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(X_train)\n",
    "X_train_tfidf = vect.transform(X_train)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1729x5268 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19135 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set\n",
    "X_test_tfidf = vect.transform(X_test)\n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - TFIDF: 0.9161364950838635\n"
     ]
    }
   ],
   "source": [
    "dt_classifier_tfidf = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "\n",
    "dt_classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = dt_classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy - TFIDF:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Não houve grande mudança após a lemmatização\n",
    "\n",
    "Além disso, o desempenho dos dois modelos foi bastante semelhante, com boa qualidade. Entretanto, percebe-se que o conjunto de treino e teste não é de boa qualidade. Existem diversos tweets praticamente identicos escritos em momentos diferentes, o que pode estar facilitando o treinamento e classificação do modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
